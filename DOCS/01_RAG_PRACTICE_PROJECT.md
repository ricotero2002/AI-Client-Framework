# RAG Practice Project - Sistema Multi-Estrategia con Dataset de Recetas Veganas

## ðŸ“‹ Resumen Ejecutivo

Proyecto de investigaciÃ³n que implementa y compara **6 estrategias diferentes de RAG** (Retrieval-Augmented Generation) utilizando un dataset de recetas vegetarianas. El objetivo principal es analizar el trade-off entre **calidad de respuesta**, **latencia** y **costo** en diferentes arquitecturas RAG.

**Resultado Principal**: Graph RAG obtuvo la mejor calidad (0.654) pero con 13Ã— mÃ¡s latencia que Naive RAG, revelando que el cuello de botella estÃ¡ en el **retrieval**, no en la generaciÃ³n.

---

## ðŸŽ¯ Objetivos del Proyecto

1. **Implementar 6 estrategias RAG** con complejidad creciente
2. **Evaluar objetivamente** usando DeepEval (LLM-as-a-Judge)
3. **Analizar mÃ©tricas** de calidad, latencia y costo
4. **Identificar bottlenecks** en el pipeline RAG
5. **Generar recomendaciones** para optimizaciÃ³n

---

## ðŸ“Š Dataset Utilizado

### Fuente de Datos
- **Nombre**: Vegan Recipes Dataset
- **Formato Original**: CSV
- **Procesamiento**: Convertido a Parquet para eficiencia
- **UbicaciÃ³n**: `data/processed/vegan_recipes_processed.parquet`

### Estructura del Dataset
```python
Columnas principales:
- title: Nombre de la receta
- ingredients: Lista de ingredientes
- instructions: Pasos de preparaciÃ³n
- nutrition: InformaciÃ³n nutricional (calorÃ­as, proteÃ­nas, etc.)
- tags: CategorÃ­as (sopa, ensalada, curry, etc.)
```

### EstadÃ­sticas
- **Total de recetas**: ~50 recetas veganas
- **Campos indexados**: title, ingredients, instructions, nutrition
- **Embeddings**: Generados con modelo de OpenAI `text-embedding-3-small`

### Casos de Prueba
Se crearon **6 consultas de prueba** con ground truth para evaluaciÃ³n:

```python
TEST_QUERIES = [
    {
        "query": "Menciona exactamente 3 recetas que contengan garbanzos.",
        "expected_output": "1) Chickpea & Potato Curry, 2) Roasted Curried Chickpeas..."
    },
    {
        "query": "Dame solo una receta de sopa que tenga menos de 200 calorÃ­as.",
        "expected_output": "Creamy Cauliflower Pakora Soup (135 calorÃ­as)"
    },
    # ... 4 consultas mÃ¡s
]
```

---

## ðŸ› ï¸ Estrategias RAG Implementadas

### 1. **No RAG (Baseline)**
**Archivo**: `src/rag_strategies/no_rag.py`

**DescripciÃ³n**: El modelo responde Ãºnicamente con su conocimiento pre-entrenado, sin acceso al dataset.

**ImplementaciÃ³n**:
```python
class NoRAGStrategy(BaseRAGStrategy):
    def generate_response(self, query: str) -> Dict[str, Any]:
        prompt = Prompt()
        prompt.set_system("Eres un asistente de cocina vegana.")
        prompt.set_user_input(query)
        
        response, usage = self.client.get_response(prompt)
        return {"response": response, "context": []}
```

**PropÃ³sito**: Establecer baseline para medir el valor agregado del RAG.

---

### 2. **Naive RAG**
**Archivo**: `src/rag_strategies/naive_rag.py`

**DescripciÃ³n**: ImplementaciÃ³n mÃ¡s simple de RAG: bÃºsqueda vectorial + generaciÃ³n directa.

**Pipeline**:
1. **Retrieval**: BÃºsqueda de similitud coseno en ChromaDB
2. **Generation**: Prompt con contexto recuperado

**ImplementaciÃ³n**:
```python
class NaiveRAGStrategy(BaseRAGStrategy):
    def __init__(self, top_k=3):
        self.vector_db = ChromaManager()
        self.top_k = top_k
    
    def generate_response(self, query: str):
        # Paso 1: Recuperar documentos
        results = self.vector_db.query(query, n_results=self.top_k)
        
        # Paso 2: Construir prompt
        context = "\n\n".join([doc["content"] for doc in results])
        prompt = Prompt()
        prompt.set_system("Responde usando SOLO el contexto proporcionado.")
        prompt.set_user_input(f"Contexto:\n{context}\n\nPregunta: {query}")
        
        # Paso 3: Generar respuesta
        response, usage = self.client.get_response(prompt)
        return {"response": response, "context": results}
```

**Resultados**:
- âš¡ **Latencia**: 1.4s (el mÃ¡s rÃ¡pido)
- ðŸ’° **Costo**: Muy bajo
- ðŸ“Š **Calidad**: 0.421 (inaceptable)
- âŒ **Recall**: 0.08 (pierde 92% de informaciÃ³n relevante)

**DiagnÃ³stico**: Excelente para prototipos, pero calidad insuficiente para producciÃ³n.

---

### 3. **Advanced RAG**
**Archivo**: `src/rag_strategies/advanced_rag.py`

**DescripciÃ³n**: Implementa tÃ©cnicas avanzadas de retrieval: query expansion, re-ranking y prompt engineering mejorado.

**Pipeline**:
1. **Query Expansion**: LLM genera mÃºltiples variaciones de la consulta
2. **Multi-Query Retrieval**: BÃºsqueda con cada variaciÃ³n
3. **Re-Ranking**: Cross-encoder reordena por relevancia
4. **Generation**: Prompt estructurado con few-shot examples

**ImplementaciÃ³n Detallada**:

```python
class AdvancedRAGStrategy(BaseRAGStrategy):
    def __init__(self, top_k=20):
        self.vector_db = ChromaManager()
        self.expansion_client = create_client("gemini")
        self.expansion_client.select_model("gemini-2.5-flash")
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        self.top_k = top_k
    
    def _expand_query(self, query: str) -> List[str]:
        """Genera 3 variaciones de la consulta"""
        prompt = Prompt()
        prompt.set_system("""Genera 3 variaciones de la consulta para mejorar retrieval.
        Considera: sinÃ³nimos, reformulaciones, descomposiciÃ³n en sub-preguntas.""")
        prompt.set_user_input(query)
        
        response, _ = self.expansion_client.get_response(prompt)
        # Parsear las 3 queries expandidas
        return [query] + parse_expanded_queries(response)
    
    def _rerank_documents(self, query: str, docs: List[Dict]) -> List[Dict]:
        """Re-rankea usando cross-encoder"""
        pairs = [[query, doc["content"]] for doc in docs]
        scores = self.reranker.predict(pairs)
        
        # Ordenar por score descendente
        ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, score in ranked]
    
    def generate_response(self, query: str):
        # Paso 1: Query Expansion
        expanded_queries = self._expand_query(query)
        
        # Paso 2: Multi-Query Retrieval
        all_docs = []
        for exp_query in expanded_queries:
            results = self.vector_db.query(exp_query, n_results=self.top_k)
            all_docs.extend(results)
        
        # Deduplicar
        unique_docs = deduplicate_by_content(all_docs)
        
        # Paso 3: Re-Ranking
        reranked_docs = self._rerank_documents(query, unique_docs)[:10]
        
        # Paso 4: Generation con Few-Shot
        context = format_context(reranked_docs)
        prompt = Prompt()
        prompt.set_system(ADVANCED_SYSTEM_PROMPT)
        prompt.add_few_shot_example(
            user="Dame recetas con tofu",
            assistant="AquÃ­ tienes 2 recetas con tofu: 1) Tofu Scramble..."
        )
        prompt.set_user_input(f"Contexto:\n{context}\n\nPregunta: {query}")
        
        response, usage = self.client.get_response(prompt)
        
        return {
            "response": response,
            "context": reranked_docs,
            "extra_info": {
                "expanded_queries": expanded_queries,
                "reranking_scores": [doc["rerank_score"] for doc in reranked_docs]
            }
        }
```

**Resultados**:
- ðŸ“Š **Calidad**: 0.5 (decepcionante dado el costo)
- ðŸ’° **Costo**: $0.017 (el mÃ¡s caro)
- âš ï¸ **Precision**: 0.333 (bajo ROI)
- ðŸ” **Recall**: 0.25 (mejor que Naive, pero insuficiente)

**DiagnÃ³stico**: El re-ranker no estÃ¡ optimizado. Necesita fine-tuning para dominio culinario.

---

### 4. **Agentic RAG**
**Archivo**: `src/rag_strategies/agentic_rag.py`

**DescripciÃ³n**: Agente autÃ³nomo con LangGraph que decide dinÃ¡micamente quÃ© herramientas usar.

**Arquitectura**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Planner   â”‚ â”€â”€> Analiza query y decide estrategia
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€> Tool 1: vector_search(query, top_k)
       â”œâ”€â”€> Tool 2: filter_by_nutrition(calories, protein)
       â”œâ”€â”€> Tool 3: get_recipe_by_name(name)
       â””â”€â”€> Tool 4: list_recipes_by_tag(tag)
```

**ImplementaciÃ³n con LangGraph**:

```python
from langgraph.graph import StateGraph, END

class AgenticRAGStrategy(BaseRAGStrategy):
    def __init__(self, max_iterations=15):
        self.vector_db = ChromaManager()
        self.max_iterations = max_iterations
        self.graph = self._build_graph()
    
    def _build_graph(self):
        # Definir herramientas
        tools = [
            Tool(
                name="vector_search",
                func=self.vector_db.query,
                description="Busca recetas por similitud semÃ¡ntica"
            ),
            Tool(
                name="filter_nutrition",
                func=self._filter_by_nutrition,
                description="Filtra por calorÃ­as, proteÃ­nas, etc."
            )
        ]
        
        # Crear agente ReAct
        workflow = StateGraph(AgentState)
        workflow.add_node("planner", self._planner_node)
        workflow.add_node("executor", self._executor_node)
        workflow.add_node("synthesizer", self._synthesizer_node)
        
        workflow.add_edge("planner", "executor")
        workflow.add_conditional_edges(
            "executor",
            self._should_continue,
            {"continue": "planner", "finish": "synthesizer"}
        )
        workflow.add_edge("synthesizer", END)
        
        return workflow.compile()
    
    def _planner_node(self, state: AgentState):
        """Decide quÃ© herramienta usar"""
        prompt = Prompt()
        prompt.set_system(AGENTIC_PLANNER_PROMPT)
        prompt.set_tools(self.tools)
        prompt.set_user_input(state["query"])
        
        response, _ = self.client.get_response(prompt)
        tool_call = parse_tool_call(response)
        
        return {"tool_call": tool_call, "iteration": state["iteration"] + 1}
    
    def _executor_node(self, state: AgentState):
        """Ejecuta la herramienta seleccionada"""
        tool_name = state["tool_call"]["name"]
        tool_args = state["tool_call"]["args"]
        
        result = self.tools[tool_name](**tool_args)
        
        return {"observations": state["observations"] + [result]}
    
    def generate_response(self, query: str):
        initial_state = {
            "query": query,
            "observations": [],
            "iteration": 0
        }
        
        final_state = self.graph.invoke(initial_state)
        return {
            "response": final_state["final_answer"],
            "context": final_state["observations"]
        }
```

**Resultados**:
- âœ… **Fidelidad**: 1.0 (no alucina)
- âŒ **Relevancia Contextual**: 0.190 (trae mucho "ruido")
- ðŸ“Š **Calidad General**: 0.6
- ðŸ”„ **Iteraciones Promedio**: 3-5

**DiagnÃ³stico**: Excelente para tareas multi-paso, pero necesita mejor filtrado de contexto.

---

### 5. **Graph RAG**
**Archivo**: `src/rag_strategies/graph_rag.py`

**DescripciÃ³n**: Utiliza un Knowledge Graph (Neo4j) para entender relaciones entre entidades (recetas, ingredientes, nutriciÃ³n).

**Arquitectura del Grafo**:
```
(Recipe) -[:CONTAINS]-> (Ingredient)
(Recipe) -[:HAS_NUTRITION]-> (Nutrition)
(Recipe) -[:TAGGED_AS]-> (Tag)
(Ingredient) -[:PAIRS_WITH]-> (Ingredient)
```

**Pipeline**:
1. **Graph Construction**: LLM extrae entidades y relaciones
2. **Hybrid Retrieval**: Combina bÃºsqueda vectorial + Cypher queries
3. **Path Finding**: Encuentra conexiones entre conceptos
4. **Generation**: Contexto enriquecido con relaciones

**ImplementaciÃ³n**:

```python
from neo4j import GraphDatabase
from llama_index.core import KnowledgeGraphIndex

class GraphRAGStrategy(BaseRAGStrategy):
    def __init__(self, graph_index, graph_retriever, top_k=10):
        self.graph_index = graph_index
        self.graph_retriever = graph_retriever
        self.top_k = top_k
    
    def generate_response(self, query: str):
        # Paso 1: Determinar tipo de consulta
        query_type = self._classify_query(query)
        
        if query_type == "relationship":
            # Usar Cypher query para relaciones
            results = self.graph_retriever.query(query)
        else:
            # Usar retrieval hÃ­brido
            results = self.graph_retriever.retrieve(query, top_k=self.top_k)
        
        # Paso 2: Enriquecer con contexto de grafo
        enriched_context = self._enrich_with_graph(results)
        
        # Paso 3: Generar respuesta
        prompt = Prompt()
        prompt.set_system(GRAPH_RAG_SYSTEM_PROMPT)
        prompt.set_user_input(f"Contexto:\n{enriched_context}\n\nPregunta: {query}")
        
        response, usage = self.client.get_response(prompt)
        
        return {"response": response, "context": results}
    
    def _enrich_with_graph(self, results):
        """Agrega relaciones del grafo al contexto"""
        enriched = []
        for result in results:
            # Obtener nodos relacionados
            related = self.graph_index.get_related_nodes(result["node_id"])
            enriched.append({
                "content": result["content"],
                "relationships": related
            })
        return enriched
```

**ConstrucciÃ³n del Grafo**:

```python
# setup_graph.py
from src.graph_db.graph_builder import GraphBuilder

builder = GraphBuilder(neo4j_manager, llm_model="gemini-2.5-pro")

# Extraer entidades y relaciones con LLM
for recipe in recipes_df.iterrows():
    entities = builder.extract_entities(recipe)
    relationships = builder.extract_relationships(recipe, entities)
    
    # Insertar en Neo4j
    builder.add_to_graph(entities, relationships)
```

**Resultados**:
- ðŸ† **Calidad**: 0.654 (la mejor)
- âœ… **Fidelidad**: 0.958 (casi perfecta)
- âš ï¸ **Latencia**: 18.6s Â± 12s (13Ã— mÃ¡s lento que Naive)
- ðŸ’° **Costo**: Alto (mÃºltiples llamadas LLM)

**DiagnÃ³stico**: Ideal para investigaciÃ³n offline y reportes complejos. Necesita optimizaciÃ³n de latencia.

---

## ðŸ“ˆ EvaluaciÃ³n con DeepEval

### Framework de EvaluaciÃ³n
**Herramienta**: DeepEval (LLM-as-a-Judge)  
**Archivo**: `src/evaluation/evaluator.py`

### MÃ©tricas Implementadas

#### 1. **Contextual Precision**
```python
ContextualPrecisionMetric(threshold=0.5, model=eval_llm)
```
**DefiniciÃ³n**: Â¿Los documentos recuperados son relevantes para la query?  
**FÃ³rmula**: `relevant_docs / total_retrieved_docs`

#### 2. **Contextual Recall**
```python
ContextualRecallMetric(threshold=0.5, model=eval_llm)
```
**DefiniciÃ³n**: Â¿Se recuperÃ³ toda la informaciÃ³n necesaria del ground truth?  
**FÃ³rmula**: `info_in_context / info_in_ground_truth`

#### 3. **Contextual Relevancy**
```python
ContextualRelevancyMetric(threshold=0.5, model=eval_llm)
```
**DefiniciÃ³n**: Â¿El contexto recuperado es relevante (sin ruido)?  
**FÃ³rmula**: `relevant_sentences / total_sentences_in_context`

#### 4. **Faithfulness**
```python
FaithfulnessMetric(threshold=0.5, model=eval_llm)
```
**DefiniciÃ³n**: Â¿La respuesta estÃ¡ respaldada por el contexto (no alucina)?  
**FÃ³rmula**: `claims_supported / total_claims`

#### 5. **Answer Relevancy**
```python
AnswerRelevancyMetric(threshold=0.5, model=eval_llm)
```
**DefiniciÃ³n**: Â¿La respuesta es relevante a la pregunta original?  
**FÃ³rmula**: Similitud semÃ¡ntica entre query y respuesta

### ImplementaciÃ³n del Evaluador

```python
class RAGEvaluator:
    def __init__(self):
        self.eval_llm = DeepEvalCustomLLM(
            provider="openai",
            model_name="gpt-4o-mini"
        )
        
        self.metrics = {
            "contextual_precision": ContextualPrecisionMetric(...),
            "contextual_recall": ContextualRecallMetric(...),
            "contextual_relevancy": ContextualRelevancyMetric(...),
            "faithfulness": FaithfulnessMetric(...),
            "answer_relevancy": AnswerRelevancyMetric(...)
        }
    
    def evaluate_response(self, query, response, context, expected_output):
        test_case = LLMTestCase(
            input=query,
            actual_output=response,
            retrieval_context=context,
            expected_output=expected_output
        )
        
        results = {}
        for name, metric in self.metrics.items():
            metric.measure(test_case)
            results[name] = {
                "score": metric.score,
                "reason": metric.reason
            }
        
        results["overall_score"] = mean([m.score for m in self.metrics.values()])
        return results
```

### EjecuciÃ³n de Experimentos

```python
# run_all_experiments.py
runner = ExperimentRunner(strategies, TEST_QUERIES)
results = runner.run_experiments()

# Resultados guardados en JSON
{
    "timestamp": "2026-01-30T15:30:00",
    "strategy": "graph_rag",
    "query": "Menciona 3 recetas con garbanzos",
    "response": "1) Chickpea & Potato Curry...",
    "quality_metrics": {
        "contextual_precision": {"score": 0.8, "reason": "..."},
        "contextual_recall": {"score": 0.33, "reason": "..."},
        "faithfulness": {"score": 0.95, "reason": "..."},
        "overall_score": 0.654
    },
    "latency_ms": 18600,
    "cost_usd": 0.015
}
```

---

## ðŸ“Š Resultados Comparativos

### Tabla de Resultados

| Estrategia | Calidad | Latencia | Costo | Recall | Fidelidad | Uso Ideal |
|-----------|---------|----------|-------|--------|-----------|-----------|
| **Graph RAG** | 0.654 ðŸ† | 18.6s âš ï¸ | Alto | 0.33 | 0.958 âœ… | InvestigaciÃ³n offline |
| **Agentic RAG** | 0.600 | 8.5s | Medio | 0.25 | 1.0 âœ… | Tareas multi-paso |
| **Advanced RAG** | 0.500 | 5.2s | $0.017 ðŸ’° | 0.25 | 1.0 âœ… | Necesita optimizaciÃ³n |
| **Naive RAG** | 0.421 âŒ | 1.4s âš¡ | Bajo | 0.08 âŒ | 0.85 | Prototipos |
| **No RAG** | 0.300 | 0.8s | Muy bajo | 0.0 | 0.5 | Baseline |

### AnÃ¡lisis de Bottlenecks

#### ðŸ”´ **Problema Principal: RETRIEVAL**
Todas las estrategias sufren de **bajo Contextual Recall** (0.08 - 0.33):
- **Significado**: El sistema pierde 67-92% de informaciÃ³n relevante
- **Causa RaÃ­z**: 
  - Dataset pequeÃ±o (~50 recetas)
  - Embeddings genÃ©ricos (no fine-tuned para dominio culinario)
  - Consultas complejas que requieren razonamiento multi-hop

#### ðŸŸ¢ **Fortaleza: GENERATION**
Los LLMs estÃ¡n funcionando perfectamente:
- **Agentic & Advanced**: Fidelidad 1.0 (no alucinan)
- **ConclusiÃ³n**: El cuello de botella NO es la generaciÃ³n

---

## ðŸ’° AnÃ¡lisis de Costos

### Desglose por Estrategia

```python
# Costos estimados por 1000 queries
{
    "naive_rag": "$2.50",
    "advanced_rag": "$17.00",  # âš ï¸ MÃ¡s caro
    "agentic_rag": "$8.50",
    "graph_rag": "$12.00"  # Estimado (usa herramientas externas)
}
```

### Optimizaciones de Costo Implementadas

1. **Modelo de ExpansiÃ³n**: `gemini-2.5-flash` (barato) para query expansion
2. **Modelo de GeneraciÃ³n**: `DEFAULT_MODEL` configurable
3. **Caching**: Embeddings cacheados en ChromaDB
4. **Batch Processing**: MÃºltiples queries en paralelo

---

## ðŸ”® Mejoras Futuras

### ðŸš‘ Prioridad 1: Arreglar Retrieval (URGENTE)

#### OpciÃ³n A: Cambiar Modelo de Embeddings
```python
# Actual
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Propuesto
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
# O fine-tuning especÃ­fico para recetas
```

#### OpciÃ³n B: Aumentar Dataset
- Agregar 500+ recetas veganas
- Incluir mÃ¡s variaciones de ingredientes
- Enriquecer metadatos nutricionales

#### OpciÃ³n C: Hybrid Search
```python
# Combinar bÃºsqueda vectorial + keyword search
results_vector = chroma.query(query, n_results=20)
results_bm25 = bm25_index.search(query, k=20)
results_final = reciprocal_rank_fusion([results_vector, results_bm25])
```

### ðŸ§¹ Prioridad 2: Limpieza de Contexto

#### Mejorar Re-Ranker
```python
# Actual: Cross-encoder genÃ©rico
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# Propuesto: Fine-tuned para recetas
reranker = CrossEncoder('custom-recipe-reranker')
# Entrenar con pares (query, receta_relevante)
```

#### Filtrado de Ruido en Agentic RAG
```python
def _filter_observations(self, observations):
    """Elimina observaciones redundantes o irrelevantes"""
    unique = deduplicate_by_semantic_similarity(observations)
    relevant = [obs for obs in unique if relevance_score(obs) > 0.7]
    return relevant[:5]  # Top 5
```

### âš¡ Prioridad 3: OptimizaciÃ³n de Graph RAG

#### Pre-CÃ¡lculo de Grafos
```python
# Cachear subgrafos frecuentes
frequent_patterns = [
    "recetas_con_ingrediente",
    "recetas_bajo_calorias",
    "recetas_alto_proteina"
]

for pattern in frequent_patterns:
    subgraph = extract_subgraph(pattern)
    cache.set(pattern, subgraph)
```

#### ParalelizaciÃ³n de Queries
```python
import asyncio

async def parallel_graph_retrieval(queries):
    tasks = [graph_retriever.query_async(q) for q in queries]
    results = await asyncio.gather(*tasks)
    return merge_results(results)
```

---

## ðŸ› ï¸ TecnologÃ­as Utilizadas

### Core Framework
- **Python**: 3.10+
- **LangChain**: Abstracciones RAG
- **LangGraph**: OrquestaciÃ³n de agentes

### Vector Database
- **ChromaDB**: Almacenamiento de embeddings
- **Persistencia**: Local en `data/chroma_db/`

### Graph Database
- **Neo4j**: Knowledge Graph
- **LlamaIndex**: IntegraciÃ³n con Neo4j

### LLMs
- **OpenAI**: GPT-4o-mini (evaluaciÃ³n)
- **Google Gemini**: gemini-2.5-flash (query expansion), gemini-2.5-pro (graph construction)

### EvaluaciÃ³n
- **DeepEval**: Framework LLM-as-a-Judge
- **MÃ©tricas**: Precision, Recall, Relevancy, Faithfulness

### Utilities
- **Pandas**: Procesamiento de datos
- **Pydantic**: ValidaciÃ³n de schemas
- **python-dotenv**: GestiÃ³n de variables de entorno

---

## ðŸ“ Estructura del Proyecto

```
rag_practice_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ vegan_recipes.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ vegan_recipes_processed.parquet
â”‚   â””â”€â”€ chroma_db/                    # Vector DB persistente
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ load_dataset.py           # Carga y preprocesamiento
â”‚   â”œâ”€â”€ vector_db/
â”‚   â”‚   â”œâ”€â”€ chroma_manager.py         # Wrapper ChromaDB
â”‚   â”‚   â””â”€â”€ setup_chroma.py           # InicializaciÃ³n
â”‚   â”œâ”€â”€ graph_db/
â”‚   â”‚   â”œâ”€â”€ neo4j_manager.py          # ConexiÃ³n Neo4j
â”‚   â”‚   â”œâ”€â”€ graph_builder.py          # ConstrucciÃ³n del grafo
â”‚   â”‚   â””â”€â”€ graph_retriever.py        # Queries Cypher
â”‚   â”œâ”€â”€ rag_strategies/
â”‚   â”‚   â”œâ”€â”€ base_strategy.py          # Clase abstracta
â”‚   â”‚   â”œâ”€â”€ no_rag.py
â”‚   â”‚   â”œâ”€â”€ naive_rag.py
â”‚   â”‚   â”œâ”€â”€ advanced_rag.py
â”‚   â”‚   â”œâ”€â”€ agentic_rag.py
â”‚   â”‚   â””â”€â”€ graph_rag.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluator.py              # DeepEval integration
â”‚   â”‚   â””â”€â”€ analyzer.py               # AnÃ¡lisis de resultados
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config_loader.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                     # ConfiguraciÃ³n global
â”œâ”€â”€ results/                          # Resultados de experimentos
â”‚   â””â”€â”€ experiment_results_*.json
â”œâ”€â”€ run_all_experiments.py            # Script principal
â”œâ”€â”€ setup_graph.py                    # ConstrucciÃ³n del grafo
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸš€ InstalaciÃ³n y Uso

### 1. InstalaciÃ³n

```bash
# Clonar repositorio
cd rag_practice_project

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
cat > .env << EOF
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
EOF
```

### 2. Preparar Datos

```bash
# Cargar dataset
python src/data/load_dataset.py

# Crear vector database
python src/vector_db/setup_chroma.py

# Construir knowledge graph (opcional, solo para Graph RAG)
python setup_graph.py
```

### 3. Ejecutar Experimentos

```bash
# Correr todas las estrategias
python run_all_experiments.py

# Resultados en: results/experiment_results_YYYYMMDD_HHMMSS.json
```

### 4. Analizar Resultados

```bash
# Generar visualizaciones y resumen
python src/evaluation/analyzer.py
```

---

## ðŸ“ Conclusiones

### Hallazgos Clave

1. **Graph RAG es el ganador en calidad** (0.654) pero requiere optimizaciÃ³n de latencia
2. **El bottleneck estÃ¡ en el retrieval**, no en la generaciÃ³n
3. **Advanced RAG tiene bajo ROI** - necesita mejor re-ranker
4. **Agentic RAG tiene fidelidad perfecta** pero trae ruido contextual

### Recomendaciones

- **Para producciÃ³n**: Usar Naive RAG optimizado (mejor latencia/costo)
- **Para investigaciÃ³n**: Graph RAG con pre-cÃ¡lculo de subgrafos
- **Para tareas complejas**: Agentic RAG con filtrado mejorado

### Lecciones Aprendidas

1. **Dataset pequeÃ±o limita todas las estrategias** - invertir en datos de calidad
2. **Embeddings genÃ©ricos no capturan semÃ¡ntica de dominio** - considerar fine-tuning
3. **LLM-as-a-Judge (DeepEval) es confiable** para evaluaciÃ³n objetiva
4. **Trade-off calidad/latencia es inevitable** - elegir segÃºn caso de uso

---

**Proyecto realizado como prÃ¡ctica de arquitecturas RAG avanzadas.**  
**Fecha**: Enero 2026  
**DuraciÃ³n**: 3 meses
