# AI Client Framework & Agentic Systems

**Framework completo y extensible para orquestar LLMs, construir agentes autÃ³nomos, y desarrollar sistemas de IA de producciÃ³n**

Un ecosistema integrado que incluye: wrapper unificado multi-provider, sistema de prompts estructurados, agentes con supervisiÃ³n humana, pipelines de generaciÃ³n de contenido, servidores MCP, sistemas RAG avanzados, y herramientas de evaluaciÃ³n.

---

## ğŸ¯ Proyectos Principales

### 1. **ğŸ¤– Human-in-the-Loop IDL Agent** â­â­â­
**Agente AutÃ³nomo con SupervisiÃ³n Humana y Sistema de Rollback**

Agente inteligente basado en LangGraph y MCP que ejecuta comandos de terminal de forma autÃ³noma, con aprobaciÃ³n humana para operaciones peligrosas, sistema de backup/restore automÃ¡tico, y capacidad de aprendizaje de errores pasados.

**CaracterÃ­sticas:**
- âœ… Arquitectura de doble agente (Planner + Executor)
- âœ… SupervisiÃ³n humana selectiva (solo para operaciones unsafe)
- âœ… Backup automÃ¡tico antes de cambios destructivos
- âœ… Rollback inteligente si el usuario rechaza cambios
- âœ… Aprendizaje de errores (Golden Dataset)
- âœ… ProtecciÃ³n contra alucinaciones de paths
- âœ… AuditorÃ­a completa en `execution_audit.log`

**TecnologÃ­as:** LangGraph, MCP, Gemini 2.5, Pydantic

ğŸ“‚ **Directorio:** `Human_IDL/`  
ğŸ“– **README:** [Human_IDL/README.md](Human_IDL/README.md)

**Ejemplo de Uso:**
```bash
python client.py server_terminal.py
```

---

### 2. **ğŸ”§ AI Client Framework (Core)** â­â­â­
**Wrapper Unificado Multi-Provider con Prompt Engineering Avanzado**

Framework robusto para orquestar mÃºltiples LLMs (OpenAI, Gemini, Anthropic) con soporte nativo para prompts estructurados, chat persistente, evaluaciÃ³n de prompts, y tracing avanzado.

#### **Nuevas Funcionalidades del Wrapper**

##### **Prompt Class - Soporte para Chats**
```python
from prompt import Prompt

# Crear prompt con historial de conversaciÃ³n
prompt = Prompt()
prompt.set_system("Eres un asistente experto en Python")

# Agregar mensajes al historial
prompt.add_user_message("Â¿QuÃ© es una lista?")
prompt.add_assistant_message("Una lista es una estructura de datos...")
prompt.add_user_message("Dame un ejemplo")

# Obtener respuesta (mantiene contexto)
response, _ = client.get_response(prompt)
```

**CaracterÃ­sticas del Chat:**
- âœ… Historial de conversaciÃ³n completo
- âœ… Soporte para mensajes de herramientas (tool messages)
- âœ… LÃ­mite configurable de mensajes en contexto
- âœ… Compatible con todos los providers

##### **Gemini Client - Cambio AutomÃ¡tico de Modelo**
```python
from gemini_client import GeminiClient

client = GeminiClient()
client.select_model('gemini-2.5-flash')

# Si el modelo alcanza rate limit o estÃ¡ sobrecargado,
# automÃ¡ticamente cambia a un modelo alternativo similar
response, _ = client.get_response(prompt)
# âš ï¸  Rate limit hit for gemini-2.5-flash. Trying fallback models...
# ğŸ”„ Attempting with gemini-2.0-flash-exp...
# âœ… Success with gemini-2.0-flash-exp!
```

**Sistema de Fallbacks:**
- ğŸ”„ Fallbacks inteligentes basados en pricing similar
- ğŸ”„ DetecciÃ³n automÃ¡tica de rate limits (429) y sobrecarga (503)
- ğŸ”„ Cambio permanente al modelo alternativo si funciona
- ğŸ”„ Top 5 alternativas ordenadas por cercanÃ­a de precio

##### **Otras Mejoras del Wrapper**

**Structured Output con Pydantic:**
```python
from pydantic import BaseModel

class BookInfo(BaseModel):
    title: str
    author: str
    summary: str

prompt = Prompt()
prompt.set_output_schema(BookInfo)
response, _ = client.get_response(prompt)

# ValidaciÃ³n automÃ¡tica
is_valid, data, error = prompt.validate_response(response)
```

**Tool Support:**
```python
# Agregar herramientas al prompt
prompt.set_tools(gemini_tools)

# Convertir herramientas LangChain a formato Gemini
from prompt import convert_langchain_tool_to_gemini
gemini_tools = [convert_langchain_tool_to_gemini(t) for t in lc_tools]
```

**Template Variables:**
```python
prompt.set_user_input("Analiza este texto: [[text]]")
prompt.set_variable("text", "Hello world")
```

**File Attachments:**
```python
prompt.attach_image("screenshot.png", description="Error screenshot")
prompt.attach_pdf("document.pdf")
```

#### **Core Features**

1. **Multi-Provider Unified API**
   - OpenAI (Responses API)
   - Google Gemini (SDK 2.0)
   - Anthropic Claude

2. **Structured Prompt Engine**
   - System instructions
   - Few-shot examples
   - Templates con `[[variables]]`
   - Esquemas de salida estructurados (Pydantic)
   - Tracking de uso y costos
   - Versionado y mejora automÃ¡tica

3. **Chat System con Persistencia**
   - Persistencia en SQLite
   - CompresiÃ³n automÃ¡tica de contexto
   - Tracking de modelo y prompt por mensaje
   - Historial completo de conversaciones

4. **Prompt Evaluation System (Evals)**
   - Golden examples (test cases)
   - EvaluaciÃ³n automÃ¡tica con LLM
   - Feedback humano
   - Mejora automÃ¡tica de prompts
   - Versionado de prompts

5. **Observability (LangSmith)**
   - IntegraciÃ³n nativa con LangSmith
   - Tracing detallado de tokens, modelos y proveedores

6. **Advanced Pricing & Counting**
   - GestiÃ³n centralizada de mÃ¡s de 60 modelos
   - Pricing detallado (input, output, cached)

**Archivos Core:**
- `base_client.py` - Abstract Base Class
- `client_factory.py` - Factory para instanciaciÃ³n dinÃ¡mica
- `openai_client.py` - Cliente OpenAI
- `gemini_client.py` - Cliente Gemini con fallbacks
- `prompt.py` - Motor de prompts estructurados (47KB, 1331 lÃ­neas)
- `chat.py` - Sistema de chat con persistencia
- `database.py` - ORM y gestiÃ³n de base de datos
- `prompt_evaluator.py` - Motor de evaluaciÃ³n
- `config.py` - ConfiguraciÃ³n de modelos y pricing

**Ejemplos:**
- `examples/chat_example.py` - Chat bÃ¡sico
- `examples/prompt_tracking_example.py` - Tracking de uso
- `examples/comedian_eval_example.py` - EvaluaciÃ³n completa
- `interactive_chat_test.py` - Chat interactivo

---

### 3. **ğŸ“Š LangGraph Content Generation Pipeline** â­â­
**Sistema de GeneraciÃ³n de Contenido con ParalelizaciÃ³n y Feedback Loops**

Workflow avanzado de generaciÃ³n de contenido que demuestra patrones de orquestaciÃ³n complejos: Map-Reduce, Conditional Routing, Feedback Loops y Structured Output.

**CaracterÃ­sticas:**
- âœ… **Map-Reduce Pattern**: Genera mÃºltiples secciones en paralelo (~60% mÃ¡s rÃ¡pido)
- âœ… **Feedback Loop**: Editor revisa y reescribe automÃ¡ticamente
- âœ… **Structured Output**: ValidaciÃ³n con Pydantic
- âœ… **LangSmith Tracing**: Observabilidad completa

**Pipeline:**
1. **Ideation**: Genera Ã¡ngulo Ãºnico para el tema
2. **Outline**: Estructura con 3-5 secciones (structured output)
3. **Writing**: Genera secciones en paralelo (Map)
4. **Assembler**: Une las piezas (Reduce)
5. **Editor**: Revisa y mejora (Feedback Loop)

ğŸ“‚ **Directorio:** `langGraph/`  
ğŸ“– **README:** [langGraph/README.md](langGraph/README.md)

**Ejemplo de Uso:**
```bash
python langgraph_chaining.py
```

---

### 4. **ğŸ” NeMo Defense Bot** â­â­
**Sistema de Defensa para LLMs con Guardrails Multicapa**

Sistema de guardrails implementado con NVIDIA NeMo Guardrails para proteger modelos de lenguaje contra ataques adversariales, jailbreaks, inyecciones y exposiciÃ³n de PII.

**CaracterÃ­sticas:**
- âœ… **DetecciÃ³n de Jailbreaks**: NVIDIA NeMo Guard API
- âœ… **ModeraciÃ³n de Contenido**: Nemotron Safety Guard 8B
- âœ… **Control de TÃ³picos**: RestricciÃ³n de dominios
- âœ… **ProtecciÃ³n de PII**: GLiNER para enmascaramiento automÃ¡tico
- âœ… **DetecciÃ³n de Inyecciones**: SQL, XSS, Template, Code
- âœ… **Guardrails Personalizados**: Regex patterns, topic blockers

**Resultados de EvaluaciÃ³n:**
- **Garak (PromptInject)**: 100% de bloqueo (DEFCON 5)
- **Moderation Eval**: 6/6 casos bloqueados correctamente

ğŸ“‚ **Directorio:** `nemo_defense_bot/`  
ğŸ“– **README:** [nemo_defense_bot/README.md](nemo_defense_bot/README.md)

**Ejemplo de Uso:**
```bash
.\start_nemo_server.ps1
```

---

### 5. **ğŸŒ MCP Servers Suite** â­â­

#### **5.1 GitHub PR Review + Notion**
Servidor MCP para anÃ¡lisis automÃ¡tico de Pull Requests con integraciÃ³n a Notion.

**CaracterÃ­sticas:**
- âœ… Fetch automÃ¡tico de PRs de GitHub
- âœ… AnÃ¡lisis de diffs lÃ­nea por lÃ­nea
- âœ… CreaciÃ³n de documentaciÃ³n en Notion
- âœ… Compatible con Claude Desktop

ğŸ“‚ **Directorio:** `MCP/PR_Review/`  
ğŸ“– **README:** [MCP/PR_Review/README.md](MCP/PR_Review/README.md)

**Herramientas MCP:**
- `fetch_pr(repo_owner, repo_name, pr_number)`: Obtiene cambios del PR
- `create_notion_page(title, content)`: Crea pÃ¡gina en Notion

#### **5.2 Multi-MCP Server con FastAPI**
Servidor FastAPI que expone mÃºltiples servidores MCP vÃ­a HTTP.

**CaracterÃ­sticas:**
- âœ… MÃºltiples servidores MCP en una app
- âœ… Endpoints HTTP independientes
- âœ… GestiÃ³n unificada de lifecycle
- âœ… FÃ¡cil extensiÃ³n con nuevos servidores

ğŸ“‚ **Directorio:** `MCP/Multi_mcp/`  
ğŸ“– **README:** [MCP/Multi_mcp/README.md](MCP/Multi_mcp/README.md)

**Servidores Incluidos:**
- `/echo/mcp`: Herramientas de ejemplo (echo, reverse)
- `/math/mcp`: Operaciones matemÃ¡ticas (add, multiply)

**Ejemplo de Uso:**
```bash
python main.py
curl -X POST http://localhost:8000/math/mcp/call \
  -d '{"tool": "add_tool", "arguments": {"a": 10, "b": 32}}'
```

---

### 6. **ğŸ“š RAG Practice Project** â­â­
**Sistema RAG Multi-Estrategia con EvaluaciÃ³n Comparativa**

ImplementaciÃ³n completa de mÃºltiples estrategias RAG (Naive, Advanced, Agentic, Graph) con sistema de evaluaciÃ³n y comparaciÃ³n de performance.

**Estrategias Implementadas:**
- **Naive RAG**: Retrieval bÃ¡sico + generaciÃ³n
- **Advanced RAG**: Query expansion + re-ranking + prompt engineering
- **Agentic RAG**: Agente con herramientas (retrieve, search, calculate)
- **Graph RAG**: Knowledge Graph con Neo4j

**CaracterÃ­sticas:**
- âœ… EvaluaciÃ³n automÃ¡tica con mÃ©tricas (Faithfulness, Relevancy, etc.)
- âœ… ComparaciÃ³n de estrategias
- âœ… AnÃ¡lisis de trade-offs (calidad vs velocidad)
- âœ… VisualizaciÃ³n de resultados

ğŸ“‚ **Directorio:** `rag_practice_project/`  
ğŸ“– **README:** [rag_practice_project/README.md](rag_practice_project/README.md)

**Ejemplo de Uso:**
```bash
python run_all_experiments.py
python compare_systems.py
```

---

### 7. **ğŸ¯ Gbeder System** â­
**Sistema de Benchmarking de Agentes con MCP**

Sistema completo para evaluar agentes de IA usando el benchmark GAIA, con integraciÃ³n de herramientas MCP (Tavily search, calculadora, etc.).

**CaracterÃ­sticas:**
- âœ… Benchmark GAIA (General AI Assistants)
- âœ… IntegraciÃ³n con Tavily para bÃºsqueda web
- âœ… Herramientas MCP personalizadas
- âœ… AnÃ¡lisis de resultados y mÃ©tricas

ğŸ“‚ **Directorio:** `gbeder_system/`  
ğŸ“– **README:** [gbeder_system/README.md](gbeder_system/README.md)

---

## ğŸ“ Estructura del Repositorio

```
IA/
â”œâ”€â”€ ğŸ¤– Human_IDL/                    # Agente autÃ³nomo con supervisiÃ³n humana
â”‚   â”œâ”€â”€ client.py                    # Agente principal (LangGraph)
â”‚   â”œâ”€â”€ server_terminal.py           # Servidor MCP
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ“Š langGraph/                    # Pipelines de generaciÃ³n de contenido
â”‚   â”œâ”€â”€ langgraph_chaining.py        # Pipeline con Map-Reduce
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ” nemo_defense_bot/             # Sistema de guardrails para LLMs
â”‚   â”œâ”€â”€ config/                      # ConfiguraciÃ³n de guardrails
â”‚   â”œâ”€â”€ eval_outputs/                # Resultados de evaluaciones
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸŒ MCP/                          # Servidores MCP
â”‚   â”œâ”€â”€ PR_Review/                   # GitHub PR + Notion
â”‚   â”œâ”€â”€ Multi_mcp/                   # Multi-server FastAPI
â”‚   â””â”€â”€ Custom_Client/               # Cliente MCP personalizado
â”‚
â”œâ”€â”€ ğŸ“š rag_practice_project/         # Sistema RAG multi-estrategia
â”‚   â”œâ”€â”€ src/rag_strategies/          # Implementaciones de RAG
â”‚   â”œâ”€â”€ results/                     # Resultados de experimentos
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ¯ gbeder_system/                # Benchmarking de agentes
â”‚   â”œâ”€â”€ agents.py                    # Agentes con MCP
â”‚   â”œâ”€â”€ eval.py                      # Sistema de evaluaciÃ³n
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ğŸ”§ Core Framework/               # Wrapper de LLMs
â”‚   â”œâ”€â”€ base_client.py               # Abstract base class
â”‚   â”œâ”€â”€ client_factory.py            # Factory pattern
â”‚   â”œâ”€â”€ openai_client.py             # Cliente OpenAI
â”‚   â”œâ”€â”€ gemini_client.py             # Cliente Gemini (con fallbacks)
â”‚   â”œâ”€â”€ prompt.py                    # Motor de prompts (47KB)
â”‚   â”œâ”€â”€ chat.py                      # Sistema de chat
â”‚   â”œâ”€â”€ database.py                  # Persistencia SQLite
â”‚   â”œâ”€â”€ prompt_evaluator.py          # EvaluaciÃ³n de prompts
â”‚   â””â”€â”€ config.py                    # ConfiguraciÃ³n y pricing
â”‚
â”œâ”€â”€ ğŸ“ examples/                     # Ejemplos de uso
â”‚   â”œâ”€â”€ chat_example.py
â”‚   â”œâ”€â”€ prompt_tracking_example.py
â”‚   â”œâ”€â”€ comedian_eval_example.py
â”‚   â””â”€â”€ test_improvement.py
â”‚
â”œâ”€â”€ ğŸ› ï¸ Herramientas/
â”‚   â”œâ”€â”€ compare_prompts.py           # ComparaciÃ³n de prompts
â”‚   â”œâ”€â”€ interactive_chat_test.py     # Chat interactivo
â”‚   â””â”€â”€ prueba_modelos.py            # Testing de modelos
â”‚
â”œâ”€â”€ .env                             # Variables de entorno
â”œâ”€â”€ requirements.txt                 # Dependencias
â””â”€â”€ README.md                        # Este archivo
```

---

## ğŸš€ Quick Start

### 1. InstalaciÃ³n

```bash
# Clonar repositorio
git clone <repository-url>
cd IA

# Crear entorno virtual
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt
```

### 2. ConfiguraciÃ³n

Crear archivo `.env` en la raÃ­z:
```env
# LLM APIs
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
ANTHROPIC_API_KEY=...

# Opcional: LangSmith Tracing
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__...
LANGCHAIN_PROJECT=my-project

# Opcional: MCP Servers
GITHUB_TOKEN=ghp_...
NOTION_API_KEY=secret_...
NOTION_PAGE_ID=...

# Opcional: NeMo Guardrails
NVIDIA_API_KEY=nvapi-...
```

### 3. Uso BÃ¡sico del Framework

```python
from client_factory import create_client
from prompt import Prompt

# Crear cliente
client = create_client('gemini')
client.select_model('gemini-2.5-flash')

# Crear prompt
prompt = Prompt()
prompt.set_system("Eres un asistente Ãºtil.")
prompt.set_user_input("Â¿QuÃ© es Python?")

# Obtener respuesta
response, usage = client.get_response(prompt)
print(response)

# Ver costos
cost = client.estimate_cost(usage.prompt_tokens, usage.completion_tokens)
print(f"Costo: ${cost.total_cost:.6f}")
```

---

## ğŸ“š GuÃ­as de Uso Detalladas

### ğŸ”¹ Structured Outputs con Pydantic

```python
from pydantic import BaseModel, Field

class BookInfo(BaseModel):
    title: str
    author: str
    summary: str = Field(description="Resumen breve del libro")

prompt = (
    Prompt()
    .set_system("Eres un experto bibliotecario.")
    .set_user_input("Dame informaciÃ³n sobre 'Rayuela'")
    .set_output_schema(BookInfo)
)

response_json, usage = client.get_response(prompt)
book = BookInfo.model_validate_json(response_json)
print(book.title, book.author)
```

### ğŸ”¹ Chat con Persistencia

```python
from chat import ChatSession

# Crear nueva conversaciÃ³n
chat = ChatSession(title="Mi Chat", max_messages=10)

# Configurar prompt
prompt = Prompt()
prompt.set_system("Eres un asistente experto en Python.")
prompt.save()

# ConversaciÃ³n
chat.add_message('user', 'Â¿QuÃ© es una lista?')
response = chat.get_response(client, prompt)
print(response)

# Cargar conversaciÃ³n existente
chat2 = ChatSession.load(chat.conversation_id)
```

### ğŸ”¹ Prompt Tracking y EstadÃ­sticas

```python
# Crear y guardar prompt
prompt = Prompt()
prompt.set_system("Eres un revisor de cÃ³digo.")
prompt.save()

# Usar el prompt
response, usage = client.get_response(prompt)

# Guardar estadÃ­sticas de uso
cost = client.estimate_cost(usage.prompt_tokens, usage.completion_tokens)
prompt.save_usage(
    model=client.current_model,
    input_tokens=usage.prompt_tokens,
    output_tokens=usage.completion_tokens,
    response=response,
    cost=cost.total_cost
)

# Ver estadÃ­sticas
stats = prompt.get_usage_stats()
print(f"Total llamadas: {stats['total_calls']}")
print(f"Costo total: ${stats['total_cost']:.6f}")
```

### ğŸ”¹ Sistema de EvaluaciÃ³n de Prompts

```python
from prompt import Prompt
from eval_database import get_eval_db
from prompt_evaluator import PromptEvaluator

# 1. Crear prompt
comedian = Prompt()
comedian.set_system("Sos un comediante argentino...")
comedian.save()

# 2. Agregar golden examples
db = get_eval_db()
db.add_test_case(
    prompt_id=comedian.get_id(),
    input="HacÃ© un chiste sobre el subte",
    expected_output="El subte es el Ãºnico lugar...",
    category="transporte"
)

# 3. Ejecutar evaluaciÃ³n
evaluator = PromptEvaluator(eval_client)
test_cases = db.get_test_cases(comedian.get_id())
results = evaluator.batch_evaluate(comedian, test_cases, test_client)

# 4. Ver reporte
report = evaluator.generate_report(results)
print(f"Score promedio: {report['avg_score']:.2f}")
```

### ğŸ”¹ LangSmith Tracing

```python
# Crear cliente con tracing
client = create_client('gemini', langsmith=True)

# Todas las llamadas se tracean automÃ¡ticamente
response, usage = client.get_response(prompt)

# Ver en LangSmith dashboard:
# - Tokens usados
# - Costo
# - Latencia
# - Prompts completos
# - Respuestas
```

---

## ğŸ“Š Modelos Soportados

### OpenAI
- `gpt-5-nano`, `gpt-5-mini`, `gpt-4.5-preview`
- `o1`, `o1-mini`, `o3-mini`
- `gpt-4o`, `gpt-4o-mini`, `gpt-4-turbo`

### Google Gemini
- `gemini-2.0-flash-exp`, `gemini-2.0-flash-lite`
- `gemini-2.5-flash`, `gemini-2.5-pro`
- `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-1.5-flash-8b`

### Anthropic Claude
- `claude-3-5-sonnet-20241022`
- `claude-3-5-haiku-20241022`
- `claude-3-opus-20240229`

**Pricing completo en:** `config.py`

---

## ğŸ› ï¸ Herramientas Incluidas

### `compare_prompts.py`
Compara estadÃ­sticas de uso entre todos tus prompts:
- Costos totales y promedios
- Tokens por llamada
- ComparaciÃ³n por modelo
- EstadÃ­sticas detalladas

### `interactive_chat_test.py`
Chat interactivo para testing:
- Crear o cargar conversaciones
- Configurar prompts
- Comandos: `stats`, `history`, `clear`, `exit`

---

## ğŸ”§ Arquitectura

### PatrÃ³n de Herencia para Tracing
- **Clientes base** (`OpenAIClient`, `GeminiClient`): Livianos, sin decoradores
- **Clientes Smith** (`OpenAIClientSmith`, `GeminiClientSmith`): Con decoradores `@traceable` y metadatos

### Base de Datos
- **SQLite** para persistencia
- **SQLAlchemy ORM** para modelos
- **Tablas**: `conversations`, `messages`, `prompts`, `prompt_usage`, `test_cases`, `evaluations`, `prompt_versions`

### EvaluaciÃ³n de Prompts
- **Structured Output** con Pydantic para scoring confiable
- **Feedback humano** opcional para mejorar precisiÃ³n
- **Mejora automÃ¡tica** usando LLM para analizar fallas

---

## ğŸ“– Ejemplos de Uso

| Ejemplo | DescripciÃ³n | Archivo |
|---------|-------------|---------|
| Chat bÃ¡sico | Sistema de chat con persistencia | `examples/chat_example.py` |
| Prompt tracking | Tracking de uso y costos | `examples/prompt_tracking_example.py` |
| EvaluaciÃ³n completa | Sistema de evals con golden examples | `examples/comedian_eval_example.py` |
| Mejora de prompts | Mejora automÃ¡tica con feedback | `examples/test_improvement.py` |
| Chat interactivo | Herramienta de testing | `interactive_chat_test.py` |
| ComparaciÃ³n | Comparar prompts y costos | `compare_prompts.py` |

---

## ğŸš€ Roadmap

### Framework Core
- [ ] Soporte para mÃ¡s providers (Cohere, AI21, etc.)
- [ ] Streaming de respuestas
- [ ] Batch processing optimizado
- [ ] Cache distribuido con Redis

### Agentes
- [ ] Multi-agent orchestration
- [ ] Herramientas de bÃºsqueda web integradas
- [ ] Soporte para cÃ³digo ejecutable
- [ ] IntegraciÃ³n con bases de datos

### RAG
- [ ] Hybrid search (keyword + semantic)
- [ ] Multi-modal RAG (imÃ¡genes, videos)
- [ ] Adaptive retrieval strategies
- [ ] Query routing automÃ¡tico

### MCP
- [ ] MÃ¡s servidores MCP (Jira, Linear, Slack, etc.)
- [ ] Auto-discovery de servidores
- [ ] Dashboard de monitoreo
- [ ] Versioning de herramientas

### EvaluaciÃ³n
- [ ] MÃ¡s mÃ©tricas de evaluaciÃ³n
- [ ] A/B testing de prompts
- [ ] Regression testing automÃ¡tico
- [ ] Benchmark suite completo

---