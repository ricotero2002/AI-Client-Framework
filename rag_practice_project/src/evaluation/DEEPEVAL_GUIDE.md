# Integraci√≥n de DeepEval - Gu√≠a de Uso

## ‚úÖ Implementaci√≥n Completada

Se ha refactorizado exitosamente el sistema de evaluaci√≥n RAG para usar **DeepEval** en lugar de prompts manuales.

### Archivos Creados/Modificados

1. **`src/evaluation/deepeval_llm_wrapper.py`** (NUEVO)
   - Wrapper personalizado que integra DeepEval con nuestro `LLMClientWrapper`
   - Permite usar Gemini/OpenAI para las evaluaciones sin configuraci√≥n adicional

2. **`src/evaluation/evaluator.py`** (REFACTORIZADO)
   - Usa las 5 m√©tricas de DeepEval: Contextual Precision, Recall, Relevancy, Faithfulness, Answer Relevancy
   - Cada m√©trica retorna `{"score": float, "reason": str}` para debugging mejorado
   - Eliminados ~400 l√≠neas de c√≥digo de prompts manuales

3. **`src/evaluation/analyzer.py`** (REFACTORIZADO)
   - Aplana estructura de DeepEval en DataFrame
   - **Nueva visualizaci√≥n**: Radar Chart para las 5 m√©tricas
   - **Nueva secci√≥n en reportes**: An√°lisis de fallos con explicaciones de DeepEval

4. **`src/evaluation/test_queries_example.py`** (NUEVO)
   - Ejemplos de c√≥mo estructurar queries con Ground Truth

---

## üìã Formato de Datos Requerido

### IMPORTANTE: Estructura de test_queries

El `ExperimentRunner` ahora requiere que `test_queries` sea una lista de **diccionarios** con este formato:

```python
test_queries = [
    {
        "query": "¬øC√≥mo preparar hummus?",
        "expected_output": "Para hacer hummus necesitas garbanzos cocidos, tahini, lim√≥n..."
    },
    {
        "query": "¬øQu√© recetas tienen quinoa?",
        "expected_output": "Bowl de quinoa con frijoles, ensalada de quinoa..."
    }
]
```

**Campos:**
- `query` (str): La pregunta del usuario
- `expected_output` (str): La respuesta esperada (Ground Truth)

### ¬øQu√© es el Ground Truth (expected_output)?

Es la **respuesta ideal/esperada** que el sistema deber√≠a generar. DeepEval usa esto para:

1. **Contextual Recall**: Verificar si el contexto recuperado contiene la informaci√≥n necesaria
2. **Faithfulness**: Detectar si la respuesta alucina informaci√≥n no presente en el contexto
3. **Answer Relevancy**: Comparar la relevancia de la respuesta generada vs. la esperada

### Si NO tienes Ground Truth

Si no tienes respuestas esperadas preparadas, puedes usar un placeholder gen√©rico:

```python
test_queries = [
    {"query": "¬øC√≥mo hacer hummus?", "expected_output": "Respuesta detallada sobre la consulta."},
    {"query": "Recetas con quinoa", "expected_output": "Respuesta detallada sobre la consulta."}
]
```

**Nota**: Esto reduce la precisi√≥n de las m√©tricas, pero permite ejecutar DeepEval.

---

## üöÄ C√≥mo Usar

### 1. Actualizar run_all_experiments.py

Cambia tus `TEST_QUERIES` actuales (que son solo strings) por el nuevo formato:

```python
# ANTES (solo strings)
TEST_QUERIES = [
    "¬øC√≥mo preparar hummus?",
    "Dame recetas con quinoa"
]

# DESPU√âS (diccionarios con expected_output)
TEST_QUERIES = [
    {
        "query": "¬øC√≥mo preparar hummus?",
        "expected_output": "Para hacer hummus necesitas garbanzos cocidos, tahini, jugo de lim√≥n, ajo y aceite de oliva. Procesa todos los ingredientes hasta obtener una consistencia cremosa."
    },
    {
        "query": "Dame recetas con quinoa",
        "expected_output": "Bowl de quinoa con frijoles negros, ensalada de quinoa mediterr√°nea, y quinoa con verduras asadas son opciones populares."
    }
]
```

### 2. Ejecutar Experimentos

```bash
python run_all_experiments.py
```

El output ahora incluir√°:
- Scores de las 5 m√©tricas DeepEval
- Reasons (explicaciones) cuando hay fallos
- Radar chart mostrando balance entre m√©tricas
- Secci√≥n de an√°lisis de fallos en el reporte markdown

### 3. Revisar Resultados

**Visualizaciones generadas:**
- `radar_chart.png` (NUEVO) - Balance entre las 5 m√©tricas
- `retrieval_vs_generation.png` - Comparaci√≥n Retrieval vs Generation
- `diagnostic_heatmap.png` - Heatmap de todas las m√©tricas
- `hallucination_analysis.png` - Scatter plot Recall vs Faithfulness
- `cost_vs_quality.png`
- `latency_vs_quality.png`

**Reporte markdown:**
- Incluye nueva secci√≥n "üîç AN√ÅLISIS DE FALLOS"
- Muestra los 3 peores casos de cada m√©trica con explicaciones

---

## üîß Configuraci√≥n

### Modelo de Evaluaci√≥n

El modelo usado para evaluaciones se configura en `src/utils/config_loader.py`:

```python
EVALUATION_LLM_PROVIDER = "gemini"  # o "openai"
EVALUATION_MODEL = "gemini-1.5-flash"
```

DeepEval usar√° este modelo para todas las m√©tricas.

### Thresholds de M√©tricas

Actualmente configurados en 0.5 (50%). Puedes ajustarlos en `evaluator.py`:

```python
self.contextual_precision = ContextualPrecisionMetric(
    threshold=0.7,  # Cambiar aqu√≠
    model=self.deepeval_llm,
    include_reason=True
)
```

---

## üìä Interpretaci√≥n de M√©tricas

### Retrieval Metrics

| M√©trica | Qu√© mide | Score bajo indica |
|---------|----------|-------------------|
| **Contextual Precision** | Chunks relevantes al inicio | Reranking deficiente |
| **Contextual Recall** | % de info del GT en contexto | Embeddings malos o top_k bajo |
| **Contextual Relevancy** | % de contenido relevante | Mucho ruido en el contexto |

### Generation Metrics

| M√©trica | Qu√© mide | Score bajo indica |
|---------|----------|-------------------|
| **Faithfulness** | Sin alucinaciones | LLM inventa informaci√≥n |
| **Answer Relevancy** | Respuesta aborda la query | Prompt mal dise√±ado |

---

## üêõ Troubleshooting

### Error: "expected_output is required"

**Causa**: Est√°s pasando `test_queries` como lista de strings.

**Soluci√≥n**: Convierte a formato de diccionarios:

```python
# Si tienes esto:
queries = ["query1", "query2"]

# Convi√©rtelo as√≠:
test_queries = [
    {"query": q, "expected_output": "Respuesta detallada."}
    for q in queries
]
```

### Error: "DeepEvalBaseLLM not found"

**Causa**: DeepEval no est√° instalado.

**Soluci√≥n**:

```bash
pip install deepeval
```

### Visualizaciones no se ven bien

**Causa**: Falta matplotlib o seaborn.

**Soluci√≥n**:

```bash
pip install matplotlib seaborn
```

---

## üìù Pr√≥ximos Pasos Recomendados

1. **Crear Ground Truth dataset**: Dedica tiempo a escribir respuestas esperadas de calidad para tus queries de prueba
2. **Ajustar thresholds**: Experimenta con diferentes valores seg√∫n tus requisitos de calidad
3. **Analizar reasons**: Lee las explicaciones de DeepEval para entender por qu√© fallan ciertas queries
4. **Iterar en estrategias**: Usa el radar chart para identificar qu√© aspecto mejorar (retrieval vs generation)

---

## üéØ Ventajas vs. Sistema Anterior

| Aspecto | Antes | Ahora |
|---------|-------|-------|
| **C√≥digo** | ~600 l√≠neas de prompts manuales | M√©tricas probadas de DeepEval |
| **Debugging** | Solo scores num√©ricos | Scores + explicaciones detalladas |
| **Reproducibilidad** | Depend√≠a de quality de prompts | Framework estandarizado |
| **Mantenibilidad** | Dif√≠cil ajustar prompts | Configuraci√≥n simple con thresholds |
| **Visualizaci√≥n** | 6 gr√°ficos | 7 gr√°ficos + radar chart |

---

## üìö Referencias

- **DeepEval Docs**: https://docs.confident-ai.com/
- **RAG Triad Framework**: M√©tricas industria para evaluar RAG
- **Contextual Metrics**: Usan Ground Truth para validaci√≥n m√°s rigurosa
