# MigraciÃ³n de Ollama a GPT-4o en Graph RAG

## ğŸ¯ Resumen

Se migrÃ³ el sistema de Graph RAG de usar **Ollama (llama2)** a **GPT-4o via client_factory** para la extracciÃ³n de entidades y expansiÃ³n de sinÃ³nimos.

## âœ… Cambios Realizados

### 1. Nuevo Wrapper LLM para LlamaIndex

**Archivo:** [`src/graph_db/llamaindex_llm_wrapper.py`](file:///c:/Users/Agustin/Desktop/Agustin/IA/rag_practice_project/src/graph_db/llamaindex_llm_wrapper.py)

- Implementa `CustomLLM` de LlamaIndex
- Integra `client_factory` (OpenAI/Gemini clients)
- Soporta LangSmith tracing
- Compatible con PropertyGraphIndex

**Uso:**
```python
from src.graph_db.llamaindex_llm_wrapper import create_llm_for_llamaindex

llm = create_llm_for_llamaindex(
    provider="openai",
    model="gpt-4o",
    temperature=0.0,
    langsmith=True  # Habilita tracing
)
```

### 2. ActualizaciÃ³n de GraphBuilder

**Archivo:** [`src/graph_db/graph_builder.py`](file:///c:/Users/Agustin/Desktop/Agustin/IA/rag_practice_project/src/graph_db/graph_builder.py)

**Cambios:**
- âŒ Removido: `from llama_index.llms.ollama import Ollama`
- âœ… Agregado: `from src.graph_db.llamaindex_llm_wrapper import create_llm_for_llamaindex`
- âŒ Removido parÃ¡metro: `ollama_model`
- âœ… Agregados parÃ¡metros: `llm_provider`, `llm_model`, `langsmith`

**Antes:**
```python
builder = GraphBuilder(
    ollama_model="llama2"
)
```

**Ahora:**
```python
builder = GraphBuilder(
    llm_provider="openai",
    llm_model="gpt-4o",
    langsmith=False  # o True para tracing
)
```

### 3. ActualizaciÃ³n de setup_graph.py

**Archivo:** [`setup_graph.py`](file:///c:/Users/Agustin/Desktop/Agustin/IA/rag_practice_project/setup_graph.py)

**Cambios:**
```python
# Antes
builder = GraphBuilder(
    neo4j_manager=neo4j_manager,
    show_progress=True
)

# Ahora
builder = GraphBuilder(
    neo4j_manager=neo4j_manager,
    llm_provider="openai",
    llm_model="gpt-4o",
    langsmith=False,  # Cambiar a True para LangSmith
    show_progress=True
)
```

### 4. ActualizaciÃ³n de ConfiguraciÃ³n

**Archivo:** [`config/config.py`](file:///c:/Users/Agustin/Desktop/Agustin/IA/rag_practice_project/config/config.py#L66-L93)

**Nuevas variables:**
```python
# Provider y modelo para Graph RAG
GRAPH_EXTRACTION_LLM_PROVIDER = os.getenv("GRAPH_EXTRACTION_LLM_PROVIDER", "openai")
GRAPH_EXTRACTION_MODEL = os.getenv("GRAPH_EXTRACTION_MODEL", "gpt-4o")
```

**Nota:** Ollama config se marca como OBSOLETO pero se mantiene por compatibilidad.

### 5. GraphRetriever (Sin Cambios)

**Archivo:** [`src/graph_db/graph_retriever.py`](file:///c:/Users/Agustin/Desktop/Agustin/IA/rag_practice_project/src/graph_db/graph_retriever.py)

No requiere cambios - usa automÃ¡ticamente el LLM configurado en el `PropertyGraphIndex`.

---

## ğŸ”§ ConfiguraciÃ³n Necesaria

### Variables de Entorno (.env)

```bash
# OpenAI API Key (necesario para GPT-4o)
OPENAI_API_KEY=tu_openai_api_key_aqui

# Neo4j Aura
NEO4J_URI=neo4j+s://tu-instancia.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=tu_password
NEO4J_DATABASE=neo4j

# Graph RAG Configuration
GRAPH_EXTRACTION_LLM_PROVIDER=openai
GRAPH_EXTRACTION_MODEL=gpt-4o

# LangSmith (opcional, para tracing)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=tu_langsmith_key
```

---

## ğŸš€ Uso Actualizado

### ConstrucciÃ³n del Grafo

```bash
# Con GPT-4o (default ahora)
python setup_graph.py --sample 20

# Con muestra mÃ¡s grande
python setup_graph.py --sample 100
```

### ProgramÃ¡tico

```python
from src.graph_db.graph_builder import GraphBuilder
from src.graph_db.neo4j_manager import Neo4jManager

# Crear builder con GPT-4o
builder = GraphBuilder(
    llm_provider="openai",
    llm_model="gpt-4o",
    langsmith=True,  # Habilitar tracing
    show_progress=True
)

# Preparar y construir
documents = builder.prepare_documents(df)
index = builder.build_graph(documents, reset=True)
```

### Con Otros Modelos

```python
# Usar GPT-4o-mini (mÃ¡s rÃ¡pido, mÃ¡s barato)
builder = GraphBuilder(
    llm_provider="openai",
    llm_model="gpt-4o-mini",
    langsmith=False
)

# Usar Gemini
builder = GraphBuilder(
    llm_provider="gemini",
    llm_model="gemini-2.0-flash",
    langsmith=False
)
```

---

## ğŸ’° ComparaciÃ³n de Costos

### Ollama (Antes)
- **Costo monetario:** $0 (modelo local)
- **Costo computacional:** Alto uso de CPU/RAM
- **Velocidad:** Depende del hardware local
- **Calidad:** Variable segÃºn modelo

### GPT-4o (Ahora)
- **Costo monetario:** ~$2.50 / 1M input tokens, ~$10 / 1M output tokens
- **Costo computacional:** Ninguno (API)
- **Velocidad:** RÃ¡pida y consistente
- **Calidad:** Excelente para extracciÃ³n de entidades

### Ejemplo de Costo Real

Para **100 recetas**:
- Tokens estimados: ~50,000 (input) + ~20,000 (output)
- Costo aproximado: **$0.33**

Para **1,000 recetas**:
- Costo aproximado: **$3.30**

---

## âš™ï¸ Ventajas de la MigraciÃ³n

### 1. **Calidad Superior**
- GPT-4o es mucho mejor para extracciÃ³n de entidades
- Identifica relaciones mÃ¡s complejas
- Menos errores en parsing

### 2. **Velocidad Consistente**
- No depende del hardware local
- Procesamiento paralelo en la nube
- Sin variabilidad por carga del sistema

### 3. **LangSmith Integration**
- Tracing completo de extracciones
- Debug de queries complejas
- AnÃ¡lisis de costos precisos

### 4. **Flexibilidad**
- FÃ¡cil cambiar a GPT-4o-mini (mÃ¡s barato)
- FÃ¡cil cambiar a Gemini
- Mismo cÃ³digo para todos

### 5. **Sin Dependencias Locales**
- No necesitas Ollama instalado
- No necesitas descargar modelos grandes
- Funciona en cualquier mÃ¡quina

---

## ğŸ§ª Testing

### Test del Wrapper

```bash
# Test bÃ¡sico del wrapper
python src/graph_db/llamaindex_llm_wrapper.py
```

### Test de ConstrucciÃ³n

```bash
# Test con muestra muy pequeÃ±a
python setup_graph.py --sample 5
```

Esto deberÃ­a:
1. âœ… Conectar a Neo4j Aura
2. âœ… Inicializar GPT-4o via client_factory
3. âœ… Extraer entidades de 5 recetas
4. âœ… Crear nodos y relaciones en Neo4j
5. âœ… Guardar embeddings en ChromaDB

---

## ğŸ“Š VerificaciÃ³n

### 1. Verificar Wrapper Funciona

```python
from src.graph_db.llamaindex_llm_wrapper import create_llm_for_llamaindex

llm = create_llm_for_llamaindex("openai", "gpt-4o")
response = llm.complete("Extract entities: quinoa salad recipe")
print(response.text)
```

### 2. Verificar Neo4j

```cypher
// En Neo4j Browser
MATCH (n) RETURN n LIMIT 25
MATCH ()-[r]->() RETURN type(r), count(*) as count
```

### 3. Verificar LangSmith (si estÃ¡ habilitado)

- Ir a https://smith.langchain.com
- Ver traces de extracciones
- Analizar costos y latencias

---

## ğŸ› Troubleshooting

### Error: "OpenAI API Key not configured"

```bash
# Verifica que .env tenga:
OPENAI_API_KEY=sk-...
```

### Error: "ClientFactory not found"

AsegÃºrate que `client_factory.py` estÃ© en el directorio correcto:
```
c:\Users\Agustin\Desktop\Agustin\IA\client_factory.py
```

### Error: "Import Error on llamaindex_llm_wrapper"

Verifica que el path estÃ© correcto:
```python
sys.path.append(str(Path(__file__).parent.parent.parent.parent))
```

---

## ğŸ“ PrÃ³ximos Pasos

1. âœ… Test con muestra pequeÃ±a (`--sample 10`)
2. âœ… Verificar calidad de entidades extraÃ­das en Neo4j
3. âœ… Comparar costos reales vs estimados
4. âœ… Decidir si usar GPT-4o-mini para producciÃ³n (mÃ¡s barato)
5. âœ… Habilitar LangSmith si se necesita debugging

---

## ğŸ”„ Rollback (Si es necesario)

Si necesitas volver a Ollama:

1. Revierte cambios en `graph_builder.py`:
```python
from llama_index.llms.ollama import Ollama

builder = GraphBuilder(
    ollama_model="llama2"
)
```

2. AsegÃºrate que Ollama estÃ© corriendo:
```bash
ollama list
```

---

**MigraciÃ³n completada el:** 2026-01-29  
**VersiÃ³n:** Graph RAG v2.0 con GPT-4o
