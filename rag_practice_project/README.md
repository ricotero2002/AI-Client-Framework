# RAG Practice Project - Vegetarian Recipes

Este proyecto implementa y evalÃºa diferentes estrategias de RAG (Retrieval-Augmented Generation) usando un dataset de recetas vegetarianas.

## ðŸŽ¯ Objetivos

- Practicar embeddings y vector databases
- Implementar 6 estrategias diferentes de RAG
- Evaluar latencia, costo y calidad de respuestas
- Comparar resultados y analizar trade-offs

## ðŸ“Š Estrategias RAG Implementadas

1. **No RAG**: Baseline sin recuperaciÃ³n de contexto
2. **Naive RAG**: RecuperaciÃ³n â†’ Lectura â†’ GeneraciÃ³n (bÃ¡sico)
3. **Advanced RAG**: Con pre-procesamiento y re-ranking
4. **Modular RAG**: Arquitectura modular e intercambiable
5. **Agentic RAG**: RAG con agentes autÃ³nomos
6. **Graph RAG**: RAG basado en grafos de conocimiento

## ðŸ› ï¸ Stack TecnolÃ³gico

- **Embeddings**: Modelo gratuito (sentence-transformers)
- **Vector Database**: ChromaDB
- **Dataset**: Vegan Recipes Dataset (Kaggle)
- **LLM**: Configurable (OpenAI/Gemini)

## ðŸ“ Estructura del Proyecto

```
rag_practice_project/
â”œâ”€â”€ data/                      # Datos y dataset
â”œâ”€â”€ src/                       # CÃ³digo fuente
â”‚   â”œâ”€â”€ embeddings/           # Utilidades de embeddings
â”‚   â”œâ”€â”€ vector_db/            # GestiÃ³n de ChromaDB
â”‚   â”œâ”€â”€ rag_strategies/       # Implementaciones RAG
â”‚   â”œâ”€â”€ evaluation/           # Sistema de evaluaciÃ³n
â”‚   â””â”€â”€ utils/                # Utilidades generales
â”œâ”€â”€ experiments/              # Scripts de experimentos
â”œâ”€â”€ results/                  # Resultados y anÃ¡lisis
â”œâ”€â”€ config/                   # Configuraciones
â””â”€â”€ notebooks/                # Jupyter notebooks exploratorios
```

## ðŸš€ InstalaciÃ³n

```bash
pip install -r requirements.txt
```

## ðŸ“ Uso

1. **Cargar datos**:
```bash
python src/data/load_dataset.py
```

2. **Crear embeddings y vector DB**:
```bash
python src/vector_db/setup_chroma.py
```

3. **Ejecutar experimentos**:
```bash
python experiments/run_all_experiments.py
```

4. **Generar anÃ¡lisis**:
```bash
python src/evaluation/generate_report.py
```

## ðŸ“ˆ MÃ©tricas de EvaluaciÃ³n

- **Latencia**: Tiempo de respuesta
- **Costo**: Tokens utilizados y costo estimado
- **Calidad**:
  - Relevancia al prompt
  - Claridad de la respuesta
  - ConcisiÃ³n
  - Cumplimiento de requisitos
  - PrecisiÃ³n factual

## ðŸ“Š Resultados

Los resultados se guardan en `results/` con:
- MÃ©tricas comparativas
- Visualizaciones
- AnÃ¡lisis detallado por estrategia


## Modelos a definir
- Generar el prompt (Default) -> gemini-2.5-flash-lite
- Modelo de evaluacion -> gemini-2.5-flash
- Expandir prompt en advanced rag -> gemini-2.0-flash-exp
